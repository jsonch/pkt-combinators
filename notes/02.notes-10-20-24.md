1. Sharding atom type
  - a sharding atom transforms a packet of type x to a packet of type y and a "pipeline number". Should it _not_ be allowed to change the packet type? In current examples, x == y is fine.
  
2. Sharding a single packet to multiple cores
  - instead of sharding to a single core, could shard copy to multiple cores, i.e., take a _list_ of cores? (so you can have subsets of cores process each packet, and specify which cores process each packet). 
    + For example, you might want to shard between 3 groups of cores, where group 1 processes all packets where hash(pkt) = 1, group 2 processes all packets where hash(pkt) = 2, etc. 
  - could generalize current shard, or add a new combinator

3. Parallel may want to specify _which_ cores the pipeline runs on. 
    - instead of representing the "vector of streams" as a list, 
    maybe it is a dictionary (core identifier -> stream)

4. For the shard and parallel combinators, we should think about examples and what we want to do. We want to:             
  1. shard 1 packet to multiple cores instead of a single core
      + "replicate this k times" vs 
      + "assigns vectors to cores"
      + maybe separate the idea of replication from the idea of binding something to a core?
  2. run parallel pipelines on specific cores, instead of just "any n cores"

5. We want a "share" combinator -- like parallel, but with shared state 
  - thinking about parallel and share at once, there might be different degress of freedom on the granularity of sharing... 
    - e.g., cores [1, 2, 3] share state and cores [2, 4] share state. 
  - we could have a combinator that "puts multiple vectors together"
    + for example: 
      ```vector_flatten([share([1, 2, 3], foo); share([4, 5], foo)])```


6. What language are atoms implemented in?
  - one option is to implement the atoms in ebpf (or p4 or a NIC configuration). Then, when users write the atoms they really just give inputs to the source code and define the types. Our programming language boils down to a library / language that uses combinators to combine the atoms into these pipelines / stream processing functions. 
  - we might want to have multiple types of atoms for different targets: 
    + ebpf atom
    + NIC atom
    + P4 or lucid atom
  - and then, a combinator that indicates where the pipeline "moves" from P4 to NIC to ebpf. 

7. fold and unfold combinators
  - we might have combinators that do the annotation and also "fast forward" an atom on a tuple of inputs. 
  - two combinators: 
    + fold n -- take a stream of packets, generate a second stream where each packet has previous 4 appended to it
    + unfold n atom -- run atom, but first fast-forward its state with the n attached histories

  - alternately, perhaps we just express this as atoms and ask the user to give us a lemma about the equivalency of different forms of pipelines that use the atoms. Make the user tell us that "(iter x)" is equivalent to (iter annotate) >>> (shard n) >>> (parallel n (unannotate x))

8. research directions / things to do: 
  1. previous thread on composition and types of the library
  2. sharding / vector / share combinators (1 - 5)
  3. breaking down other examples into programs using the library
  4. thinking about how each of these "atoms" would be implemented in ebpf, if we are going the FFI route
